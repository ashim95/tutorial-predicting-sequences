{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Sequences\n",
    "\n",
    "This notebook will guide you through the process of training a sequence predictor. For more details about the technical lectures, see the lecture on [sequence models](https://svivek.com/teaching/structured-prediction/spring2020/lectures/sequences.html) and the associated readings. \n",
    "\n",
    "To keep things simple, we will not use any machine learning libraries and implement everything from scratch. This means that some of the code you see below may not be efficient, but is focused more on ease of understanding. (For the code below to run on your computer, you will need the following python libraries instaled: `numpy`, `tabulate`, `tqdm`, `matplotlib`, `seaborn`.)\n",
    "\n",
    "We will use the problem of extracting information from citations as an illustration. \n",
    "\n",
    "\n",
    "##  Information extraction as a sequence labeling task\n",
    "\n",
    "Our goal is to identify the title, author, year, etc from a citation. For example, consider a citation to the famous Francis Galton paper that discusses the idea of the wisdom of crowds:\n",
    "\n",
    "```\n",
    "    Francis Galton. 1907. Vox populi. Nature, 75(7):450–451.\n",
    "```\n",
    "\n",
    "Our system should fill up a table such as:\n",
    "\n",
    "| Field     | Value          |\n",
    "|-----------|----------------|\n",
    "| *Author*  | Francis Galton |\n",
    "| *Date*    | 1907           |\n",
    "| *Title*   | Vox Populi     |\n",
    "| *Journal* | Nature         |\n",
    "| *Volume*  | 75(7)          |\n",
    "| *Pages*   | 450-451        |\n",
    "\n",
    "\n",
    "In the example above, the task does not overtly look like a sequence labeling task. Let us first cast the problem into an equivalent sequence labeling problem. \n",
    "\n",
    "Each item in the **Value** column above is a span of text from the input. In other words, the task calls for extracting spans from the original text, and labeling them. Equivalently, we could percolate the label of a span to *each* word in it. For example, the labeled span $[\\text{Francis Galton .}]_{\\texttt{author}}$ would be decomposed into three instances of the same label, one for each token. This process gives us an output that is a sequence that is as long as the original text. That is, we could write our input and output as the following two sequences:\n",
    "\n",
    "\n",
    "| Index | Input token | Label     |\n",
    "|-------|-------------|-----------|\n",
    "|     0 | Francis     | `author`  |\n",
    "|     1 | Galton      | `author`  |\n",
    "|     2 | .           | `author`  |\n",
    "|     3 | 1907        | `date`    |\n",
    "|     4 | .           | `date`    |\n",
    "|     5 | Vox         | `title`   |\n",
    "|     6 | populi      | `title`   |\n",
    "|     7 | .           | `title`   |\n",
    "|     8 | Nature      | `journal` |\n",
    "|     9 | ,           | `journal` |\n",
    "|    10 | 75          | `volume`  |\n",
    "|    11 | (           | `volume`  |\n",
    "|    12 | 7           | `volume`  |\n",
    "|    13 | )           | `volume`  |\n",
    "|    14 | :           | `volume`  |\n",
    "|    15 | 450         | `pages`   |\n",
    "|    16 | –           | `pages`   |\n",
    "|    17 | 451         | `pages`   |\n",
    "|    18 | .           | `pages`   |\n",
    "\n",
    "Here we need to predict a sequence with 19 elements. At this point, we have a sequence labeling task, where the input is the citation and the output is the sequence of labels. There are other encodings of labeled spans as sequences, such as the [IOB encoding](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)). For the purpose of this notebook, we will stick to the simple encoding above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Now that we have a sequence labeling task, let us load some data. We will use a popular dataset that is based on the one used by McCallum et al (2000) and subsequently studied in various other papers \n",
    "(e.g., Grenager et al, 2005, Chang et al, 2007, and others). \n",
    "\n",
    "We have are two files `citation-train.txt` and `citation-test.txt`. The file format is:\n",
    "\n",
    "```\n",
    "[tab-separated citation words for first citation]\n",
    "[tab-separated label per word in first citation]\n",
    "[newline]\n",
    "[newline]\n",
    "[tab-separated citation words for second citation]\n",
    "[tab-separated label per word in second citation]\n",
    "[newline]\n",
    "[newline]\n",
    "...\n",
    "```\n",
    "\n",
    "\n",
    "Let us write a reader that can load a dataset and produce a dictionary with two entries: `inputs` mapping to a list of sequences of tokens, and `outputs` mapping to a list of sequences of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    \n",
    "    data = {\n",
    "        'inputs': [],\n",
    "        'outputs': []\n",
    "    }\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            input_text = lines[i].strip().split(\"\\t\")\n",
    "            output_labels = lines[i+1].strip().split(\"\\t\")\n",
    "            data['inputs'].append(input_text)\n",
    "            data['outputs'].append(output_labels)            \n",
    "            i+=4\n",
    "                \n",
    "    num_inputs = len(data['inputs'])\n",
    "    num_outputs = len(data['outputs'])\n",
    "    print(f\"Parsed {filename}\")\n",
    "    print(f\"\\tFound {num_inputs} input citations\")\n",
    "    print(f\"\\tFound {num_outputs} label sequences\")\n",
    "    if num_inputs != num_outputs:\n",
    "        print(f\"Error loading the data.\")\n",
    "        return None\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed data/citations-train.txt\n",
      "\tFound 300 input citations\n",
      "\tFound 300 label sequences\n",
      "Parsed data/citations-test.txt\n",
      "\tFound 100 input citations\n",
      "\tFound 100 label sequences\n"
     ]
    }
   ],
   "source": [
    "train = load_data('data/citations-train.txt')\n",
    "test = load_data('data/citations-test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see an example from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M . Kitsuregawa , H . Tanaka , and T . Moto-oka . Application of hash to data base machine and its architecture . New Generation Computing , 1 ( 1 ) , 1983 .\n",
      "author author author author author author author author author author author author author title title title title title title title title title title title journal journal journal journal volume volume volume volume volume date date\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(train['inputs'][0]))\n",
    "print(' '.join(train['outputs'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore this data in a slightly more readable form, let's write a helper function that accumulates the tokens associated with a field and prints a table similar to the one we saw above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "\n",
    "def pretty_print(tokens, labels):\n",
    "    \n",
    "    table = [['Field', 'Value']]\n",
    "    prev_label = None\n",
    "    value_so_far = ''\n",
    "    \n",
    "    for token, label in zip(tokens, labels):\n",
    "        if prev_label != label:\n",
    "            if prev_label is not None:\n",
    "                table.append([prev_label, value_so_far])\n",
    "            prev_label = label\n",
    "            value_so_far = token\n",
    "        else:\n",
    "            value_so_far = value_so_far + ' ' + token\n",
    "    table.append([prev_label, value_so_far])\n",
    "    display(HTML(tabulate.tabulate(table, tablefmt='html', headers='firstrow')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Field  </th><th>Value                                                          </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>author </td><td>M . Kitsuregawa , H . Tanaka , and T . Moto-oka .              </td></tr>\n",
       "<tr><td>title  </td><td>Application of hash to data base machine and its architecture .</td></tr>\n",
       "<tr><td>journal</td><td>New Generation Computing ,                                     </td></tr>\n",
       "<tr><td>volume </td><td>1 ( 1 ) ,                                                      </td></tr>\n",
       "<tr><td>date   </td><td>1983 .                                                         </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretty_print(train['inputs'][0], train['outputs'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need a list of all labels in the data. Let's get that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['author', 'booktitle', 'date', 'editor', 'institution', 'journal', 'location', 'note', 'pages', 'publisher', 'tech', 'title', 'volume']\n"
     ]
    }
   ],
   "source": [
    "all_citation_labels = set([label for example in train['outputs'] for label in example])\n",
    "\n",
    "all_citation_labels = list(all_citation_labels)\n",
    "all_citation_labels.sort()\n",
    "\n",
    "print(all_citation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Before we start off building models, it would be useful to write some code that evaluates the output of *any* predictor. This function would take two lists of sequences---the ground truth, and the prediction---and print an evaluation table. \n",
    "\n",
    "There are several things we could measure here. We could, for example, measure the overall accuracy of the predictions. We could also measure label-wise [precision, recall and the f-score](https://en.wikipedia.org/wiki/Precision_and_recall). We can compute these three measures from the number of correct predictions for a label, the total number of ground truth instances of the label, and the number of times it is predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prf(correct, total_gold, total_predicted):\n",
    "    \"\"\"\n",
    "    A helper method for computing precision, recall and f-1\n",
    "    \"\"\"\n",
    "    if total_predicted > 0:\n",
    "        precision = correct / total_predicted\n",
    "    else:\n",
    "        precision = 1.0\n",
    "\n",
    "    if total_gold > 0:\n",
    "        recall = correct / total_gold\n",
    "    else:\n",
    "        recall = 1.0\n",
    "\n",
    "    f1 = 2 * precision * recall / (precision + recall)  \n",
    "    \n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any predicted label and its ground truth, we could keep track of whether the label is correct or not by updating a counter. It is helpful to write this function separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_label(gold, prediction, counter):\n",
    "    \"\"\"\n",
    "    A helper function that updates a counter using a gold label and its prediction. \n",
    "    \n",
    "    \"\"\"\n",
    "    # 0. Record the fact that we have a label\n",
    "    counter['found_label'] += 1\n",
    "\n",
    "    # 1. Count the fact that the label was predicted\n",
    "    counter['predicted:' + prediction] += 1\n",
    "\n",
    "    # 2. Count the fact that the label showed up in the ground truth\n",
    "    counter['gold:' + gold] += 1\n",
    "\n",
    "    if gold == prediction:\n",
    "        # 3. If the prediction is correct, \n",
    "        # then, record the fact that something was correct\n",
    "        counter['correct'] +=1\n",
    "\n",
    "        # also record the fact that this particular label was correct\n",
    "        counter['correct:' + prediction] += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can implement a general purpose evaluator that takes a list of ground truth sequences and a list of predictions and prints a table that lists label-wise precision, recall and f-score. (Of course, we can choose to change what we evaluate. For example, if we care about performance on rare labels, we may want to measure the macro-average of the f-scores. It is instructive to think about alternative evaluation schemes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     11
    ]
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def evaluate(gold_labels, predictions, all_labels):\n",
    "    if len(gold_labels) != len(predictions):\n",
    "        raise Exception(f\"Number of gold sequences = {len(gold_labels)}, Number of predicted sequences = {len(predictions)}\")\n",
    "    \n",
    "    counter = Counter()\n",
    "    \n",
    "    for gold, prediction in zip(gold_labels, predictions):\n",
    "        if len(gold) != len(prediction):\n",
    "            print(gold)\n",
    "            print(prediction)\n",
    "            raise Exception(f\"Number of elements in gold sequence = {len(gold)}, Number of elements in prediction = {len(prediction)}\")\n",
    "\n",
    "        # Iterate through the elements\n",
    "        for i in range(len(gold)):\n",
    "            g = gold[i]\n",
    "            p = prediction[i]\n",
    "            \n",
    "            # Make sure that the labels are valid\n",
    "            if g not in all_labels:\n",
    "                raise Exception(\"Unknown label \" + g)\n",
    "            \n",
    "            if p not in all_labels:\n",
    "                raise Exception(\"Unknown label \" + p)\n",
    "            \n",
    "            evaluate_label(g, p, counter)\n",
    "\n",
    "      \n",
    "    # Now the counter contains counts of all the information we \n",
    "    # need to compute precision, recall and f-scores\n",
    "    \n",
    "    table = [[\"Label\", \"Correct\", \"Total gold\", \"Total predicted\", \n",
    "              \"Precision\", \"Recall\", \"F1\"]]\n",
    "    \n",
    "    for label in all_labels:\n",
    "        correct = counter['correct:' + label] or 0\n",
    "        total_gold = counter['gold:' + label] or 0\n",
    "        total_predicted = counter['predicted:' + label] or 0\n",
    "        \n",
    "        precision, recall, f1 = compute_prf(correct, total_gold, total_predicted)\n",
    "  \n",
    "        table.append([label, correct, total_gold, total_predicted, \n",
    "                      precision, recall, f1])\n",
    "        \n",
    "    overall_correct = counter['correct']\n",
    "    num_labels = counter['found_label']\n",
    "    overall_accuracy =  overall_correct / num_labels\n",
    "    table.append([\"Overall\", overall_correct, num_labels, num_labels, \n",
    "                  overall_accuracy, overall_accuracy, overall_accuracy])\n",
    "    \n",
    "    display(HTML(tabulate.tabulate(table, tablefmt='html', headers='firstrow')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful sanity check is to see what happens if we predict the most frequent label. Let's do that first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common label is author\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Label      </th><th style=\"text-align: right;\">  Correct</th><th style=\"text-align: right;\">  Total gold</th><th style=\"text-align: right;\">  Total predicted</th><th style=\"text-align: right;\">  Precision</th><th style=\"text-align: right;\">  Recall</th><th style=\"text-align: right;\">      F1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>author     </td><td style=\"text-align: right;\">      977</td><td style=\"text-align: right;\">         977</td><td style=\"text-align: right;\">             3502</td><td style=\"text-align: right;\">   0.278983</td><td style=\"text-align: right;\">1       </td><td style=\"text-align: right;\">0.436258</td></tr>\n",
       "<tr><td>booktitle  </td><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">         527</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">   1       </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">0       </td></tr>\n",
       "<tr><td>date       </td><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">         303</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">   1       </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">0       </td></tr>\n",
       "<tr><td>editor     </td><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">          95</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">   1       </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">0       </td></tr>\n",
       "<tr><td>institution</td><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">          97</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">   1       </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">0       </td></tr>\n",
       "<tr><td>journal    </td><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">         134</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">   1       </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">0       </td></tr>\n",
       "<tr><td>location   </td><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">         147</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">   1       </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">0       </td></tr>\n",
       "<tr><td>note       </td><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">          15</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">   1       </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">0       </td></tr>\n",
       "<tr><td>pages      </td><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">         146</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">   1       </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">0       </td></tr>\n",
       "<tr><td>publisher  </td><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">          63</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">   1       </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">0       </td></tr>\n",
       "<tr><td>tech       </td><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">          55</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">   1       </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">0       </td></tr>\n",
       "<tr><td>title      </td><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">         825</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">   1       </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">0       </td></tr>\n",
       "<tr><td>volume     </td><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">         118</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">   1       </td><td style=\"text-align: right;\">0       </td><td style=\"text-align: right;\">0       </td></tr>\n",
       "<tr><td>Overall    </td><td style=\"text-align: right;\">      977</td><td style=\"text-align: right;\">        3502</td><td style=\"text-align: right;\">             3502</td><td style=\"text-align: right;\">   0.278983</td><td style=\"text-align: right;\">0.278983</td><td style=\"text-align: right;\">0.278983</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def find_most_common_label(data):\n",
    "    all_labels = [label for example in data['outputs'] for label in example]\n",
    "    counter = Counter(all_labels)\n",
    "    \n",
    "    most_common = counter.most_common(1)\n",
    "    label, count = most_common[0]\n",
    "    return label\n",
    "\n",
    "def make_most_common_predictions(train, test):\n",
    "    most_common_label = find_most_common_label(train)\n",
    "    print(\"Most common label is \" + most_common_label)\n",
    "    \n",
    "    predictions = []\n",
    "    for gold in test['outputs']:\n",
    "        predictions.append([most_common_label] * len(gold))\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "most_common_predictions = make_most_common_predictions(train, test)\n",
    "\n",
    "evaluate(test['outputs'], most_common_predictions, all_citation_labels)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the most common label in the training set is `author`. Predicting that *every* token belongs to the `author` label gets us an accuracy of ~27.8% overall. Also, note that the recall for the label is 1, which is expected: every instance of the `author` label is recovered by this naive predictor.\n",
    "\n",
    "Now we have a baseline that we hope *any* reasonable learned system should outperform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a sequence model with structured Perceptron\n",
    "\n",
    "We will now design a first order sequence model for the information extraction task. In our model definitions, let us denote the input citation as ${\\bf x}$ and the output sequence as ${\\bf y} = (y_0, y_1, y_2, \\cdots, y_n)$. Here the number of labels in the output $n$ is the number of tokens in the input ${\\bf x}$. As a general rule, we will assume that the sequence ${\\bf y}$ starts with a label called $\\texttt{start}$. That is, $y_0=\\texttt{start}$. \n",
    "\n",
    "We need to design a scoring function $score({\\bf x}, {\\bf y})$ that can assign scores to the output ${\\bf y}$ for an input ${\\bf x}$.\n",
    "\n",
    "What makes this a first order sequence model is the fact that this scoring function can be decomposed into the sum of emission and transition scoring functions, denoted by $s_E$ and $s_T$ respectively. That is, we have\n",
    "\n",
    "\\begin{equation}\n",
    "score({\\bf x}, {\\bf y}) = \\sum_{i=1}^n s_E({\\bf x}, i, y_i) + \\sum_{i=1}^n s_T(y_{i-1}, y_i).\n",
    "\\end{equation}\n",
    "\n",
    "Given this definition of the scoring function, we need to specify the learning and inference algorithms. The learning algorithm will give us the scoring functions $s_E$ and $s_T$, while the inference algorithm will find the score maximizing sequence of labels for an example. We will look at the latter first, because having defined the inference algorithm, the learner is easier to implement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with the Viterbi algorithm\n",
    "\n",
    "For an input ${\\bf x}$, and given the scores emission and transition scores for this example ($s_E$ and $s_T$), the goal of inference is to find the sequence ${\\bf y}$ that maximizes the total score of the sequence $score({\\bf x}, {\\bf y})$. That is, we seek to solve:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{arg}\\max_{{\\bf y} \\in \\mathcal{Y}_{\\bf x}} \\sum_{i=1}^n s_E({\\bf x}, i, y_i) + s_T(y_{i-1}, y_i)\n",
    "\\end{equation}\n",
    "Here, we use the notation $\\mathcal{Y}_{\\bf x}$ to denote the set of all possible sequences of labels for the input ${\\bf x}$.\n",
    "\n",
    "The decomposition of the score in the form above allows us to use the Viterbi algorithm to find the score maximizing sequence. \n",
    "\n",
    "To keep the inference procedure somewhat agnostic of what actual learning algorithm and features are used, we will implement a function that takes `emission_scores` and `transition_scores` as arguments. The `emission_scores` is a list of $n$ dictionaries, one for each step in the sequence. Each dictionary is a map from the label (a string) to the corresponding emission score. The transition scores are a dictionary of dictionaries, whose entries correspond to the score for transitioning from one label to another. Note that the transition score does not depend on the time step. This is a modeling choice.\n",
    "\n",
    "Recall that the Viterbi algorithm recursively estimates the best way to get to a label $l$ at each timestep. To do so, we will build a table with the scores of such paths called `score`. If the sequence has $n$ steps and we have $m$ labels, then the `score` table is a $n \\times m$ matrix. Its entry for a step $t$ and label $l$ is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\texttt{score}[t, l] = \\begin{cases}\n",
    "s_E({\\bf x}, t, l) + s_T(\\texttt{start}, l), & t = 0 \\\\\n",
    "\\max_{l^\\prime}\\texttt{score}[t-1, l^\\prime] + s_E({\\bf x}, t, l) + s_T(l^\\prime, l), & t > 0.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "(Note that even though the function $s_E$ depends on the input ${\\bf x}$, in our implementation below, we will not consider this dependency. We will instead pre-compute all the scores for an input and call the inference procedure.)\n",
    "\n",
    "After filling up this table, we can look at the last row (i.e. $\\texttt{score}[n-1]$) and the maximum entry in that row will be score of the highest scoring sequence. To actually find the sequence, we need to keep track of which label led to the maximum at each step. We will store this information in a list called `back_pointers`.\n",
    "\n",
    "*Note*: The implementation below may not be the most efficient one because it is using dictionaries with strings in it. There are more efficient ways to implement this; optimizing this code is left as an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def inference_viterbi(emission_scores, transition_scores, all_labels, start_label='start'):\n",
    "    length = len(emission_scores)\n",
    "    num_labels = len(transition_scores)\n",
    "    score = []\n",
    "    back_pointers = []\n",
    "\n",
    "    # The base case for the first step\n",
    "    score.append({})\n",
    "    back_pointers.append({})\n",
    "    for label in all_labels:\n",
    "        score[0][label] = emission_scores[0][label] + transition_scores[start_label][label]\n",
    "    \n",
    "    # Subsequent steps\n",
    "    for t in range(1, length):\n",
    "        score.append({})\n",
    "        back_pointers.append({})\n",
    "        for label in all_labels:\n",
    "            \n",
    "            # Construct all possible transitions into this label and score them\n",
    "            all_transitions = {}\n",
    "            for prev_label in all_labels:\n",
    "                all_transitions[prev_label] = score[t-1][prev_label] + emission_scores[t][label] + transition_scores[prev_label][label]\n",
    "                \n",
    "            # Find transition from some previous label into this label that maximizes the score\n",
    "            best_prev_label = max(all_transitions, key=all_transitions.get)\n",
    "            \n",
    "            # Store the backpointer and the score\n",
    "            back_pointers[t][label] = best_prev_label\n",
    "            score[t][label] = all_transitions[best_prev_label]\n",
    "\n",
    "    # Let us construct the output from the end, following the backpointers. Afterwards, we can reverse the list\n",
    "    output = [None] * length\n",
    "    \n",
    "    # First, we find the label that maximizes the score at the the last step. This gives us the last label.\n",
    "    output[0] = max(score[length - 1], key=score[length-1].get)\n",
    "    \n",
    "    # Next, for every step (from the end), we find the back pointer that gave us the next label.\n",
    "    for t in range(1, length):\n",
    "        output[t] = back_pointers[length-t][output[t-1]]\n",
    "        \n",
    "    output.reverse()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know that this is correct? Debugging inference code can be painful because of the bookkeeping involved. It is often useful to test out the inference procedure using some made up scores. Suppose we have three possible labels `a`, `b` and `c`. \n",
    "\n",
    "For some input that admits a sequence of length four, suppose the emission scores are given by the following table:\n",
    "\n",
    "| time step $t$ | $s_E(t, a)$ | $s_E(t, b)$ | $s_E(t, c)$ |\n",
    "|---------------|-------------|-------------|-------------|\n",
    "|             1 |         0.1 |         -10 |           2 |\n",
    "|             2 |           1 |           2 |           3 |\n",
    "|             3 |          -2 |           2 |           3 |\n",
    "|             4 |           1 |           1 |         1.1 |\n",
    "\n",
    "If we did not have any transition scores (or if they were all equal), then the best thing we could do is to take the highest scoring label at each step. Going down the table, this gives us the sequence `c, c, c, c` because the label `c` is the highest scoring one at every time step.\n",
    "\n",
    "Now, suppose we have the following transition table, where every row corresponds to the label at the previous step, and the columns represent the labels at the current step: \n",
    "\n",
    "| previous label | `a` | `b` | `c` |\n",
    "|----------------|-----|-----|-----|\n",
    "| `start`        | -10 | 100 |   1 |\n",
    "| `a`            |   1 |   1 |   1 |\n",
    "| `b`            |   4 |   0 |  -1 |\n",
    "| `c`            |  -1 |  -2 |  -3 |\n",
    "\n",
    "We see that the transition scores suggest that the system *really* likes to start with the label `b`. In fact, we see that the sum of the transition score into `b` at the first step (100) and the emission score for `b` at the first step (-10) overwhelms the corresponding sum for `c`, which adds up to three.\n",
    "\n",
    "We could work out the highest scoring sequence on paper and confirm that our implementation does the right thing. Alternatively, since we have a small sequence with only three labels, we could do a brute force comparison. Let's do the latter. (Of course, it is no replacement for careful testing of the code!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi output: ['b', 'a', 'b', 'a']\n",
      "Brute force output: ['b', 'a', 'b', 'a']\n"
     ]
    }
   ],
   "source": [
    "all_labels = ['a', 'b', 'c']\n",
    "start_label = 'start'\n",
    "\n",
    "emissions = [\n",
    "    {'a': 0.1, 'b': -10, 'c': 2},\n",
    "    {'a': 1, 'b': 2, 'c': 3},\n",
    "    {'a': -2, 'b': 2, 'c': 3},\n",
    "    {'a': 1, 'b': 1, 'c': 1.1}\n",
    "]\n",
    "\n",
    "transitions = {\n",
    "    start_label: {'a': -10, 'b': 100, 'c': 1},\n",
    "    'a': {'a': 1, 'b': 1, 'c': 1},\n",
    "    'b': {'a': 4, 'b': 0, 'c': -1},\n",
    "    'c': {'a': -1, 'b': -2,'c': -3}\n",
    "}\n",
    "\n",
    "viterbi_output = inference_viterbi(emissions, transitions, all_labels, start_label)\n",
    "print(f\"Viterbi output: {viterbi_output}\")\n",
    "\n",
    "\n",
    "max_score = -math.inf\n",
    "for label0 in all_labels:\n",
    "    for label1 in all_labels:\n",
    "        for label2 in all_labels:\n",
    "            for label3 in all_labels:\n",
    "                score = (emissions[0][label0] + transitions[start_label][label0] + \n",
    "                         emissions[1][label1] + transitions[label0][label1] + \n",
    "                         emissions[2][label2] + transitions[label1][label2] + \n",
    "                         emissions[3][label3] + transitions[label2][label3])\n",
    "                if score > max_score:\n",
    "                    max_score = score\n",
    "                    brute_force_best_sequence = [label0, label1, label2, label3]\n",
    "print(f\"Brute force output: {brute_force_best_sequence}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, even though the emission scores preferred that all labels were `c`, the transitions ensured that none of them were `c`.\n",
    "\n",
    "Now that we have a working inference implementation, we could move our attention learning the scoring functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning the scoring functions\n",
    "\n",
    "In our model, we have two scoring functions $s_E$ and $s_T$. We need to define both of these. To keep things simple, let us define them to be linear functions of emission features ($\\phi_E$) and transition features ($\\phi_T$) respectively. \n",
    "\n",
    "\\begin{align}\n",
    "s_E({\\bf x}, i, l) & = {\\bf w}^T \\phi_E({\\bf x}, i, l).\\\\\n",
    "s_T(l^\\prime, l) & = {\\bf w}^T \\phi_T(l^\\prime, l). \n",
    "\\end{align}\n",
    "\n",
    "Here the weight vector ${\\bf w}$ needs to be learned. In our implementation that follows, we will make the rather simple (and inefficient) choice of representing vectors by dictionaries from strings to integers. We will need to implement dot products and additions of such vectors. Let us do that next.\n",
    "\n",
    "(This simple choice is mostly to avoid using dictionaries for mapping features to indices. A more sophisticated, and realistic, implementation should use a sparse vector library for this purpose.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(v1, v2):\n",
    "    # find all the common keys between the vectors \n",
    "    common = v1.keys() & v2.keys()\n",
    "    \n",
    "    total = 0\n",
    "    for feature in common:\n",
    "        total += v1[feature] * v2[feature]\n",
    "        \n",
    "    return total\n",
    "\n",
    "def add_in_place(vector, delta, multiplier): \n",
    "    for e in delta.keys():\n",
    "        original = vector.get(e) or 0.0\n",
    "        vector[e] = original + delta[e] * multiplier\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of learning is to find the \"best\" weights ${\\bf w}$. The definition of \"best\" depends on the loss function we choose. Let us implement the structured Perceptron algorithm introduced by Collins (2002).\n",
    "\n",
    "The structured Perceptron can be seen from the point of view of loss minimization, where the loss for a labeled example $({\\bf x}, {\\bf y}^*)$ is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "L_{Perceptron}\\left({\\bf x}, {\\bf y}^*, parameters\\right) = \\max_{{\\bf y}\\in \\mathcal{Y}_{\\bf x}} score({\\bf x}, {\\bf y}) - score({\\bf x}, {\\bf y}^*).\n",
    "\\end{align}\n",
    "\n",
    "As before $\\mathcal{Y}_{\\bf x}$ represents all possible label sequences for an input ${\\bf x}$. To compute sub-gradient of this loss with respect to the parameters, we will first need to find the sequence ${\\bf y}$ that maximizes the difference and then compute the gradient with respect to it. That is, first, we need to solve \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\bf y} = \\text{arg}\\max_{{\\bf y} \\in \\mathcal{Y}_{\\bf x}} score({\\bf x},{\\bf y})\n",
    "\\end{equation}\n",
    "\n",
    "We can use our implementation of the Viterbi algorithm above for this purpose. Of course, we will need to connect our implementation of Viterbi to the scoring functions defined by the weights above. Let's do that first assuming two functions `phi_e` and `phi_t` that we will define later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, x, settings):\n",
    "    phi_e = settings['phi_e']\n",
    "    phi_t = settings['phi_t']\n",
    "    all_labels = settings['all_labels']\n",
    "    start_label = settings['start_label']\n",
    "    \n",
    "    num_steps = len(x)\n",
    "    emission_scores = []\n",
    "        \n",
    "    for step in range(num_steps):\n",
    "        emission_scores.append({})\n",
    "        for label in all_labels:\n",
    "            emission_scores[step][label] = dot_product(w, phi_e(x, step, label))\n",
    "    \n",
    "    transition_scores = {}\n",
    "    \n",
    "    prev_label = start_label\n",
    "    transition_scores[prev_label] = {}\n",
    "    for label in all_labels:\n",
    "        transition_scores[prev_label][label] = dot_product(w, phi_t(prev_label, label))\n",
    "        \n",
    "    for prev_label in all_labels:\n",
    "        transition_scores[prev_label] = {}\n",
    "        for label in all_labels:\n",
    "            transition_scores[prev_label][label] = dot_product(w, phi_t(prev_label, label))\n",
    "        \n",
    "    output = inference_viterbi(emission_scores, transition_scores, all_labels, start_label)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can update the parameters using the gradients of $score({\\bf x},\\hat{\\bf y})$ and $score({\\bf x},{\\bf y}^*)$. The gradients of the scores are exactly the features extracted from the corresponding structures. \n",
    "\n",
    "Recall that for structured Perceptron, we will need to also compute the averaged weight vectors in addition to the parameters that we are learning, and eventually return the averaged parameters at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def compute_perceptron_update(x, y_gold, y_predicted, settings):\n",
    "    num_steps = len(x)\n",
    "    if len(y_gold) != num_steps:\n",
    "        raise Exception(f\"Invalid gold structure. Expecting {num_steps} items, found {len(y_gold)}.\")\n",
    "    if len(y_predicted) != num_steps:\n",
    "        raise Exception(f\"Invalid gold structure. Expecting {num_steps} items, found {len(y_predicted)}.\")\n",
    "    \n",
    "    phi_e = settings['phi_e']\n",
    "    phi_t = settings['phi_t']\n",
    "    start_label = settings['start_label']\n",
    "    # Let us first aggregate all the features for the gold and the predicted structures\n",
    "    \n",
    "    delta = {}\n",
    "    \n",
    "    # emissions\n",
    "    for t in range(num_steps):\n",
    "        if y_gold[t] != y_predicted[t]:\n",
    "            add_in_place(delta, phi_e(x, t, y_gold[t]), 1.0)\n",
    "            add_in_place(delta, phi_e(x, t, y_predicted[t]), -1.0)\n",
    "            \n",
    "    # initial \n",
    "    if y_gold[0] != y_predicted[0]:\n",
    "        add_in_place(delta, phi_t(start_label, y_gold[0]), 1.0)\n",
    "        add_in_place(delta, phi_t(start_label, y_predicted[0]), -1.0)\n",
    "    \n",
    "    # transitions\n",
    "    for t in range(1, num_steps):\n",
    "        if y_gold[t-1] != y_predicted[t-1] or y_gold[t] != y_predicted[t]:\n",
    "            add_in_place(delta, phi_t(y_gold[t-1], y_gold[t]), 1.0)\n",
    "            add_in_place(delta, phi_t(y_predicted[t-1], y_predicted[t]), -1.0)\n",
    "\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from tqdm import trange, tqdm\n",
    "import numpy as np\n",
    "\n",
    "def learn_perceptron(train, settings):\n",
    "   \n",
    "    # For simplicity, we will store the weights as dictionaries. \n",
    "    # Of course, this is not efficient. But we won't worry about this now.\n",
    "    w = {}\n",
    "\n",
    "    # The average weights, for averaged perceptron\n",
    "    a_w = {}\n",
    "    \n",
    "    num_examples = len(train['inputs'])\n",
    "\n",
    "    for epoch_id in range(settings['num_epochs']):\n",
    "        # First shuffle the examples\n",
    "        random_indices = np.random.permutation(num_examples)\n",
    "        learning_rate = settings['initial_learning_rate']/(1 + settings['learning_rate_decay'])\n",
    "        \n",
    "        # Next iterate through them\n",
    "        num_updates_in_epoch = 0\n",
    "        for example_id in tqdm(random_indices, desc='Examples', leave=False):\n",
    "            x = train['inputs'][example_id]\n",
    "            y_gold = train['outputs'][example_id]\n",
    "\n",
    "            y_predicted = predict(w, x, settings)\n",
    "            \n",
    "            # Now, perform the perceptron update with the emission and transition features.\n",
    "            if y_gold != y_predicted:\n",
    "                delta = compute_perceptron_update(x, y_gold, y_predicted, settings)\n",
    "                add_in_place(w, delta, learning_rate)\n",
    "            \n",
    "            # Next add the current weights to the average vector\n",
    "            add_in_place(a_w, w, 1.0)\n",
    "    return a_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "The only thing that is left now is to define the feature functions `phi_e` and `phi_t`. In the spirit of keeping things simple, let us define simple indicator features for both. \n",
    "\n",
    "1. For emissions, we will use indicators for the previous token, the current token and the next token and also an indicator for whether the current word is a number. All these features are conjoined with the label.\n",
    "\n",
    "2. For transitions, we will use a single indicator feature for the previous label conjoined with the current label.\n",
    "\n",
    "Since we are multiplying the both features with the same weight vector, we need to ensure that the feature names are distinct. To do so, we will prefix all emission features with the word `emission` and all transition features with the word `transition`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def phi_e(x, t, label):\n",
    "    \n",
    "    current_token = x[t]\n",
    "    if t == 0:\n",
    "        prev_token = \"[none]\"\n",
    "    else:\n",
    "        prev_token = x[t-1]\n",
    "    \n",
    "    if t == len(x) - 1:\n",
    "        next_token = \"[none]\"\n",
    "    else:\n",
    "        next_token = x[t+1]\n",
    "    \n",
    "    input_features = [\n",
    "        'prev_token:' + prev_token,\n",
    "        'current_token:' + current_token,\n",
    "        'next_token:' + next_token,\n",
    "        'current_is_number:' + str(current_token.isnumeric())\n",
    "    ]\n",
    "    \n",
    "    features = {}\n",
    "    for f in input_features:\n",
    "        features[f\"emission:label:{label}_input:{f}\"] = 1.0\n",
    "\n",
    "    return features\n",
    "\n",
    "def phi_t(prev_label, label):\n",
    "    return {\n",
    "        f'transition:{prev_label}_{label}': 1.0\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation script\n",
    "\n",
    "Finally, it is useful to write a function that takes a learned weight vector, a test set and the various settings and evaluates the predictions of the weight vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(w, test, settings):\n",
    "    phi_e = settings['phi_e']\n",
    "    phi_t = settings['phi_t']\n",
    "    all_labels = settings['all_labels']\n",
    "    start_label = settings['start_label']\n",
    "        \n",
    "    predictions = []\n",
    "    for x in tqdm(test['inputs']):\n",
    "        y = predict(w, x, settings)\n",
    "        predictions.append(y)\n",
    "        \n",
    "    evaluate(test['outputs'], predictions, all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the pieces needed to train a model for our information extraction task. We will need to decide on the various hyper-parameters and constants we have encountered so far. Ideally, we should use cross-validation to decide on the hyperparameters. But for simplicity, let us just pick some values and run our experiment. Let us first train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                            \r"
     ]
    }
   ],
   "source": [
    "perceptron_settings = {\n",
    "    'num_epochs': 10,\n",
    "    'initial_learning_rate': 1.0,\n",
    "    'learning_rate_decay': 0.1,\n",
    "    'phi_e': phi_e,\n",
    "    'phi_t': phi_t,\n",
    "    'all_labels': all_citation_labels,\n",
    "    'start_label': 'start'\n",
    "}\n",
    "\n",
    "w_perceptron = learn_perceptron(train, perceptron_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:25<00:00,  1.46s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Label      </th><th style=\"text-align: right;\">  Correct</th><th style=\"text-align: right;\">  Total gold</th><th style=\"text-align: right;\">  Total predicted</th><th style=\"text-align: right;\">  Precision</th><th style=\"text-align: right;\">  Recall</th><th style=\"text-align: right;\">      F1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>author     </td><td style=\"text-align: right;\">      950</td><td style=\"text-align: right;\">         977</td><td style=\"text-align: right;\">             1050</td><td style=\"text-align: right;\">   0.904762</td><td style=\"text-align: right;\">0.972364</td><td style=\"text-align: right;\">0.937346</td></tr>\n",
       "<tr><td>booktitle  </td><td style=\"text-align: right;\">      477</td><td style=\"text-align: right;\">         527</td><td style=\"text-align: right;\">              524</td><td style=\"text-align: right;\">   0.910305</td><td style=\"text-align: right;\">0.905123</td><td style=\"text-align: right;\">0.907707</td></tr>\n",
       "<tr><td>date       </td><td style=\"text-align: right;\">      289</td><td style=\"text-align: right;\">         303</td><td style=\"text-align: right;\">              319</td><td style=\"text-align: right;\">   0.905956</td><td style=\"text-align: right;\">0.953795</td><td style=\"text-align: right;\">0.92926 </td></tr>\n",
       "<tr><td>editor     </td><td style=\"text-align: right;\">       18</td><td style=\"text-align: right;\">          95</td><td style=\"text-align: right;\">               20</td><td style=\"text-align: right;\">   0.9     </td><td style=\"text-align: right;\">0.189474</td><td style=\"text-align: right;\">0.313043</td></tr>\n",
       "<tr><td>institution</td><td style=\"text-align: right;\">       91</td><td style=\"text-align: right;\">          97</td><td style=\"text-align: right;\">               93</td><td style=\"text-align: right;\">   0.978495</td><td style=\"text-align: right;\">0.938144</td><td style=\"text-align: right;\">0.957895</td></tr>\n",
       "<tr><td>journal    </td><td style=\"text-align: right;\">       96</td><td style=\"text-align: right;\">         134</td><td style=\"text-align: right;\">              100</td><td style=\"text-align: right;\">   0.96    </td><td style=\"text-align: right;\">0.716418</td><td style=\"text-align: right;\">0.820513</td></tr>\n",
       "<tr><td>location   </td><td style=\"text-align: right;\">      100</td><td style=\"text-align: right;\">         147</td><td style=\"text-align: right;\">              110</td><td style=\"text-align: right;\">   0.909091</td><td style=\"text-align: right;\">0.680272</td><td style=\"text-align: right;\">0.77821 </td></tr>\n",
       "<tr><td>note       </td><td style=\"text-align: right;\">        3</td><td style=\"text-align: right;\">          15</td><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">   1       </td><td style=\"text-align: right;\">0.2     </td><td style=\"text-align: right;\">0.333333</td></tr>\n",
       "<tr><td>pages      </td><td style=\"text-align: right;\">      133</td><td style=\"text-align: right;\">         146</td><td style=\"text-align: right;\">              178</td><td style=\"text-align: right;\">   0.747191</td><td style=\"text-align: right;\">0.910959</td><td style=\"text-align: right;\">0.820988</td></tr>\n",
       "<tr><td>publisher  </td><td style=\"text-align: right;\">       54</td><td style=\"text-align: right;\">          63</td><td style=\"text-align: right;\">               66</td><td style=\"text-align: right;\">   0.818182</td><td style=\"text-align: right;\">0.857143</td><td style=\"text-align: right;\">0.837209</td></tr>\n",
       "<tr><td>tech       </td><td style=\"text-align: right;\">       48</td><td style=\"text-align: right;\">          55</td><td style=\"text-align: right;\">               52</td><td style=\"text-align: right;\">   0.923077</td><td style=\"text-align: right;\">0.872727</td><td style=\"text-align: right;\">0.897196</td></tr>\n",
       "<tr><td>title      </td><td style=\"text-align: right;\">      781</td><td style=\"text-align: right;\">         825</td><td style=\"text-align: right;\">              870</td><td style=\"text-align: right;\">   0.897701</td><td style=\"text-align: right;\">0.946667</td><td style=\"text-align: right;\">0.921534</td></tr>\n",
       "<tr><td>volume     </td><td style=\"text-align: right;\">      106</td><td style=\"text-align: right;\">         118</td><td style=\"text-align: right;\">              117</td><td style=\"text-align: right;\">   0.905983</td><td style=\"text-align: right;\">0.898305</td><td style=\"text-align: right;\">0.902128</td></tr>\n",
       "<tr><td>Overall    </td><td style=\"text-align: right;\">     3146</td><td style=\"text-align: right;\">        3502</td><td style=\"text-align: right;\">             3502</td><td style=\"text-align: right;\">   0.898344</td><td style=\"text-align: right;\">0.898344</td><td style=\"text-align: right;\">0.898344</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_model(w_perceptron, test, perceptron_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even the simple approach we have, with untuned hyperparameters, gives us what looks like fairly good performance when we measure performance at a per-label level. Let us now revisit the original example we saw above and see if our model recovers its labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Field  </th><th>Value              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>author </td><td>Francis Galton ,   </td></tr>\n",
       "<tr><td>date   </td><td>1907               </td></tr>\n",
       "<tr><td>title  </td><td>, Vox populi .     </td></tr>\n",
       "<tr><td>journal</td><td>Nature ,           </td></tr>\n",
       "<tr><td>volume </td><td>75                 </td></tr>\n",
       "<tr><td>date   </td><td>( 7 ) : 450 – 451 .</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def predict_for_example(w, x, settings):\n",
    "    prediction = predict(w, x, settings)\n",
    "    pretty_print(x, prediction)\n",
    "     \n",
    "x = ['Francis', 'Galton', ',', '1907',  ',', 'Vox', 'populi', '.', 'Nature', \n",
    "     ',', '75', '(', '7', ')', ':', '450', '–', '451', '.']\n",
    "\n",
    "predict_for_example(w_perceptron, x, perceptron_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to get some things right, but in general this doesn't seem that great. How do we improve it? Some possibilities (other than training with more data) are listed in the exercises below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the model parameters\n",
    "\n",
    "One good thing about the fact that our parameters are all dictionaries and we have a linear model is that we can examine the weights to see if they are meaningful. Let us look at the transition weights as a heatmap. (Recall that the features are of the form `transition:<prev_label>_<label>`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1038e23c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "num_labels = len(all_citation_labels)\n",
    "transitions = np.zeros((num_labels + 1, num_labels))\n",
    "\n",
    "for i, l1 in enumerate(all_citation_labels):\n",
    "    for j, l2 in enumerate(all_citation_labels):\n",
    "        transitions[i][j] = w_perceptron.get('transition:' + l1 + \"_\" + l2) or 0.0\n",
    "        \n",
    "for i, l in enumerate(all_citation_labels):\n",
    "    transitions[num_labels][i] = w_perceptron.get('transition:start_' + l) or 0.0\n",
    "\n",
    "ax = sns.heatmap(transitions, linewidth=0.5, \n",
    "                 xticklabels = all_citation_labels,\n",
    "                 yticklabels = all_citation_labels + ['start'],\n",
    "                 square=True,  cmap=\"RdYlGn\")\n",
    "\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Previous label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The positive tells us the transitions prefers the label to remain the same (unless, of course, the emissions force a change). There are a few other transitions that also have high scores. For example, the model seems to prefer to start a sequence with the `author` label. The heatmap also shows transitions that the model does not like. It is worth thinking about whether these make sense, and how we can fix them if they do not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Find better hyper-parameters for the structured Perceptron learner via cross-validation. \n",
    "\n",
    "2. Design and experiment with more features.\n",
    "\n",
    "3. Measure how long the inference step above takes, and optimize it for speed. Also, replace dictionary-based implementation of vectors with an efficient one, say, using numpy.\n",
    "\n",
    "4. Derive the gradient of the structured Perceptron loss with respect to ${\\bf w}$ and ${\\bf u}$ and show that the updates above are correct.\n",
    "\n",
    "5. Implement a structured SVM learner. For your implementation, you should use the existing implementation of inference as a sub-routine for loss-augmented inference. \n",
    "\n",
    "6. Implement a CRF learner. For the CRF, you will need to compute the expected feature vector given a model. You will have to derive and implement this inference step.\n",
    "\n",
    "7. Evaluate a higher order Markov model for the task. Implement inference for the higher order Markov model using the existing implementation of the first order inference as a sub-routine.\n",
    "\n",
    "8. Use a different input representation, say, a neural network for the emission scores. Perhaps also the transition scores.\n",
    "\n",
    "9. How would you add constraints to help the model learn and predict meaningful outputs? See Chang et al (2007) for ideas.\n",
    "\n",
    "10. Here, we have taken the original problem of populating fields from text and forced it to look like a sequence labeling problem. It is worth thinking about whether this is the only right way to frame the problem, and indeed, whether this is even a right approach. What do we lose by encoding the information extraction task in this fashion?\n",
    "\n",
    "11. Is the evaluation at the token level a meaningful metric for this task? How would you change the evaluation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* Ming-Wei Chang, Lev Ratinov, and Dan Roth. 2007. [Guiding Semi-Supervision with Constraint-Driven Learning](https://www.aclweb.org/anthology/P07-1036.pdf). In Proceedings of the 45th annual meeting of the association of computational linguistics, pages 280–287.\n",
    "\n",
    "* Michael Collins. 2002. [Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms](https://www.aclweb.org/anthology/W02-1001.pdf). In Empirical Methods in Natural Language Processing.\n",
    "\n",
    "* Trond Grenager, Dan Klein, and Christopher D. Manning. 2005. [Unsupervised learning of field segmentation models for information extraction](https://www.aclweb.org/anthology/P05-1046.pdf). In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 371–378.\n",
    "\n",
    "* John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. [Conditional random fields: Probabilistic models for segmenting and labeling sequence data](https://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&context=cis_papers). In International Conference on Machine Learning (ICML), pages 282–289.\n",
    "\n",
    "* Andrew McCallum, Dayne Freitag, and Fernando Pereira. 2000. [Maximum Entropy Markov Models for Information Extraction and Segmentation](https://pdfs.semanticscholar.org/a13e/bf46f8e200192de40c3677736378e5384140.pdf). In International Conference on Machine Learning (ICML), pages 591–598.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
